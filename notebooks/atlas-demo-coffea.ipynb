{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "3b01dae37977440b636d22625a08ccea3d4a5dd7dac9ee1bcc018aa68724fe57"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Test Coffea\n",
    "\n",
    "This will test Coffea to see if we can figure out how to use it with our code.\n",
    "\n",
    "First are the includes from coffea. This is based on the [example written by Ben](https://github.com/CoffeaTeam/coffea/blob/master/binder/servicex/ATLAS/LocalExample.ipynb)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicex import ServiceXDataset\n",
    "from coffea.processor.servicex import DataSource, FuncAdlDataset, Analysis\n",
    "from coffea.processor.servicex import LocalExecutor \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from coffea import hist, processor\n",
    "from IPython.display import display, update_display, HTML"
   ]
  },
  {
   "source": [
    "And imports connected with running servicex."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "from hist import Hist\n",
    "import mplhep as mpl\n",
    "import awkward as ak\n",
    "\n",
    "from utils import files"
   ]
  },
  {
   "source": [
    "Methods copied to help us get all leptons from the source files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_event_cuts (source: ObjectStream) -> ObjectStream:\n",
    "    '''Event level cuts for the analysis. Keep from sending data that we aren't going to need at all in the end.\n",
    "    '''\n",
    "    return (source\n",
    "        .Where(lambda e: e.trigE or e.trigM))\n",
    "def good_leptons(source: ObjectStream) -> ObjectStream:\n",
    "    '''Select out all good leptons from each event. Return their pt, eta, phi, and E, and other\n",
    "    things needed downstream.\n",
    "\n",
    "    Because uproot doesn't tie toegher the objects, we can't do any cuts at this point.\n",
    "    '''\n",
    "    return source.Select(lambda e:\n",
    "        {\n",
    "            'lep_pt': e.lep_pt,\n",
    "            'lep_eta': e.lep_eta,\n",
    "            'lep_phi': e.lep_phi,\n",
    "            'lep_E': e.lep_E,\n",
    "            'lep_ptcone30': e.lep_ptcone30,\n",
    "            'lep_etcone20': e.lep_etcone20,\n",
    "            'lep_type': e.lep_type,\n",
    "            'lep_trackd0pvunbiased': e.lep_trackd0pvunbiased,\n",
    "            'lep_tracksigd0pvunbiased': e.lep_tracksigd0pvunbiased,\n",
    "            'lep_z0': e.lep_z0,\n",
    "        }) \\\n",
    "        .AsParquetFiles('junk.parquet')"
   ]
  },
  {
   "source": [
    "Create the `func_adl` cuts to get the data. The dataset we use here doesn't matter, as long as it \"looks\" like all the datasets we are going to be processing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ServiceXSourceUpROOT('cernopendata://dummy',  files['ggH125_ZZ4lep']['treename'], backend='open_uproot')\n",
    "ds.return_qastle = True\n",
    "leptons = good_leptons(apply_event_cuts(ds))"
   ]
  },
  {
   "source": [
    "The analysis code that will apply the 4 lepton cuts and make the 4 lepton mass plot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATLAS_Higgs_4L(Analysis):\n",
    "    @staticmethod\n",
    "    def process(events):\n",
    "        import awkward as ak\n",
    "        from collections import defaultdict\n",
    "\n",
    "        sumw = defaultdict(float)\n",
    "        mass_hist = hist.Hist(\n",
    "            \"Events\",\n",
    "            hist.Cat(\"dataset\", \"Dataset\"),\n",
    "            hist.Bin(\"mass\", \"$Z_{ee}$ [GeV]\", 60, 60, 120),\n",
    "        )\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "        leptons = events.lep\n",
    "\n",
    "        # We need to look at 4 lepton events only.\n",
    "        cut = (ak.num(lep) == 4)\n",
    "\n",
    "        # Form the invar mass, plot.\n",
    "        # diele = electrons[cut][:, 0] + electrons[cut][:, 1]\n",
    "        # diele.mass\n",
    "        mass_4l = 100.0\n",
    "\n",
    "        # Fill the histogram\n",
    "        sumw[dataset] += len(events)\n",
    "        mass_hist.fill(\n",
    "            dataset=dataset,\n",
    "            mass=mass_4l,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"sumw\": sumw,\n",
    "            \"mass\": mass_hist\n",
    "        }"
   ]
  },
  {
   "source": [
    "Create the data source that we will be running against."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds(name: str, query: ObjectStream):\n",
    "    '''Create a ServiceX Datasource for a particular ATLAS Open data file\n",
    "    '''\n",
    "    datasets = [ServiceXDataset(files[name]['files'], backend_type='open_uproot')]\n",
    "    return DataSource(query=query, metadata={'dataset': name}, datasets=datasets)"
   ]
  },
  {
   "source": [
    "And run!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not a ROOT file: first four bytes are b'PAR1'\nin file C:\\Users\\gordo\\AppData\\Local\\Temp\\servicex_gordo\\data\\733a371b-a36c-4bcc-8318-f2308ed5f707\\root___eospublic.cern.ch__eos_opendata_atlas_OutreachDatasets_2020-01-22_4lep_MC_mc_345060.ggH125_ZZ4lep.4lep.root.parquet",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-00cf0a2b6422>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Why do I need run_updates_stream, why not just await on execute (which fails with async gen can't).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Perhaps something from aiostream can help here?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mrun_updates_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-00cf0a2b6422>\u001b[0m in \u001b[0;36mrun_updates_stream\u001b[1;34m(accumulator_stream)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m   \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoffea_info\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maccumulator_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoffea_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gordo\\code\\iris-hep\\coffea\\coffea\\processor\\servicex\\executor.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, analysis, datasource)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# There is an accumulate pattern in the aiostream lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfinished_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gordo\\code\\iris-hep\\coffea\\coffea\\processor\\accumulator.py\u001b[0m in \u001b[0;36masync_accumulate\u001b[1;34m(result_stream)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;32masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miadd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\pyhep-2021-SX-OpenDataDemo\\.venv\\lib\\site-packages\\aiostream\\stream\\advanced.py\u001b[0m in \u001b[0;36mbase_combine\u001b[1;34m(source, switch, ordered, task_limit)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# Get result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# End of stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\pyhep-2021-SX-OpenDataDemo\\.venv\\lib\\site-packages\\aiostream\\stream\\combine.py\u001b[0m in \u001b[0;36msmap\u001b[1;34m(source, func, *more_sources)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmore_sources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mstreamcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmore_sources\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gordo\\code\\iris-hep\\coffea\\coffea\\processor\\servicex\\executor.py\u001b[0m in \u001b[0;36mlaunch_analysis_tasks_from_stream\u001b[1;34m(self, result_file_stream, process_func)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;31m# Determine the tree name if we've not gotten it already\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtree_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[1;32mwith\u001b[0m \u001b[0muproot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                     \u001b[0mtree_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\pyhep-2021-SX-OpenDataDemo\\.venv\\lib\\site-packages\\uproot\\reading.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\u001b[0m\n\u001b[0;32m    147\u001b[0m         )\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     file = ReadOnlyFile(\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mobject_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\pyhep-2021-SX-OpenDataDemo\\.venv\\lib\\site-packages\\uproot\\reading.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"root\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    639\u001b[0m                 \"\"\"not a ROOT file: first four bytes are {0}\n\u001b[0;32m    640\u001b[0m in file {1}\"\"\".format(\n",
      "\u001b[1;31mValueError\u001b[0m: not a ROOT file: first four bytes are b'PAR1'\nin file C:\\Users\\gordo\\AppData\\Local\\Temp\\servicex_gordo\\data\\733a371b-a36c-4bcc-8318-f2308ed5f707\\root___eospublic.cern.ch__eos_opendata_atlas_OutreachDatasets_2020-01-22_4lep_MC_mc_345060.ggH125_ZZ4lep.4lep.root.parquet"
     ]
    }
   ],
   "source": [
    "analysis = ATLAS_Higgs_4L()\n",
    "executor = LocalExecutor()\n",
    "datasource = make_ds('ggH125_ZZ4lep', leptons)\n",
    "\n",
    "async def run_updates_stream(accumulator_stream):\n",
    "  global first\n",
    "  fig, axes = plt.subplots()\n",
    "  first = True\n",
    "\n",
    "  count = 0\n",
    "  async for coffea_info in accumulator_stream:\n",
    "    count += 1\n",
    "    print(count, coffea_info)\n",
    "  return coffea_info\n",
    "\n",
    "# Why do I need run_updates_stream, why not just await on execute (which fails with async gen can't).\n",
    "# Perhaps something from aiostream can help here?\n",
    "result = await run_updates_stream(executor.execute(analysis, datasource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}