{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "41c976a3581ca23dd5321440c1ba6aa61955a2d341cd42821c29af40225a7b53"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test Coffea with a DASK executor\r\n",
    "\r\n",
    "This will test Coffea to see if we can figure out how to use it with our code, but running with a local DASK executor.\r\n",
    "This doesn't make sense for this query - there is only one file, but it will test the pipeline!\r\n",
    "\r\n",
    "First are the includes from coffea. This is based on the [example written by Ben](https://github.com/CoffeaTeam/coffea/blob/master/binder/servicex/ATLAS/LocalExample.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from servicex import ServiceXDataset\r\n",
    "from coffea.processor.servicex import DataSource, Analysis\r\n",
    "from coffea.processor.servicex import DaskExecutor \r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from coffea import hist, processor\r\n",
    "from IPython.display import display, update_display, HTML"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And imports connected with running servicex."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from func_adl import ObjectStream\r\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\r\n",
    "from hist import Hist\r\n",
    "import mplhep as mpl\r\n",
    "import awkward as ak\r\n",
    "\r\n",
    "from utils import files"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Methods copied to help us get all leptons from the source files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def apply_event_cuts (source: ObjectStream) -> ObjectStream:\r\n",
    "    '''Event level cuts for the analysis. Keep from sending data that we aren't going to need at all in the end.\r\n",
    "    '''\r\n",
    "    return (source\r\n",
    "        .Where(lambda e: e.trigE or e.trigM))\r\n",
    "\r\n",
    "def good_leptons(source: ObjectStream) -> ObjectStream:\r\n",
    "    '''Select out all good leptons from each event. Return their pt, eta, phi, and E, and other\r\n",
    "    things needed downstream.\r\n",
    "\r\n",
    "    Because uproot doesn't tie toegher the objects, we can't do any cuts at this point.\r\n",
    "    '''\r\n",
    "    return source.Select(lambda e:\r\n",
    "        {\r\n",
    "            'lep_pt': e.lep_pt,\r\n",
    "            'lep_eta': e.lep_eta,\r\n",
    "            'lep_phi': e.lep_phi,\r\n",
    "            'lep_energy': e.lep_E,\r\n",
    "            'lep_charge': e.lep_charge,\r\n",
    "            'lep_ptcone30': e.lep_ptcone30,\r\n",
    "            'lep_etcone20': e.lep_etcone20,\r\n",
    "            'lep_type': e.lep_type,\r\n",
    "            'lep_trackd0pvunbiased': e.lep_trackd0pvunbiased,\r\n",
    "            'lep_tracksigd0pvunbiased': e.lep_tracksigd0pvunbiased,\r\n",
    "            'lep_z0': e.lep_z0,\r\n",
    "        })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the `func_adl` cuts to get the data. The dataset we use here doesn't matter, as long as it \"looks\" like all the datasets we are going to be processing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "ds = ServiceXSourceUpROOT('cernopendata://dummy',  files['ggH125_ZZ4lep']['treename'], backend_name='dev_uproot')\r\n",
    "ds.return_qastle = True\r\n",
    "leptons = good_leptons(apply_event_cuts(ds))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The analysis code that will apply the 4 lepton cuts and make the 4 lepton mass plot."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class ATLAS_Higgs_4L(Analysis):\r\n",
    "    @staticmethod\r\n",
    "    def process(events):\r\n",
    "        import awkward as ak\r\n",
    "        from collections import defaultdict\r\n",
    "\r\n",
    "        sumw = defaultdict(float)\r\n",
    "        mass_hist = hist.Hist(\r\n",
    "            \"Events\",\r\n",
    "            hist.Cat(\"dataset\", \"Dataset\"),\r\n",
    "            hist.Bin(\"mass\", \"$Z_{ee}$ [GeV]\", 60, 60, 120),\r\n",
    "        )\r\n",
    "\r\n",
    "        dataset = events.metadata['dataset']\r\n",
    "        leptons = events.lep\r\n",
    "\r\n",
    "        # We need to look at 4 lepton events only.\r\n",
    "        cut = (ak.num(leptons) == 4)\r\n",
    "\r\n",
    "        # Form the invar mass, plot.\r\n",
    "        # diele = electrons[cut][:, 0] + electrons[cut][:, 1]\r\n",
    "        # diele.mass\r\n",
    "        dilepton = leptons[:,0] + leptons[:,1]\r\n",
    "        mass_4l = leptons.mass\r\n",
    "\r\n",
    "        # Fill the histogram\r\n",
    "        sumw[dataset] += len(events)\r\n",
    "        print(len(events))\r\n",
    "        mass_hist.fill(\r\n",
    "            dataset=dataset,\r\n",
    "            mass=ak.flatten(mass_4l),\r\n",
    "        )\r\n",
    "        \r\n",
    "        return {\r\n",
    "            \"sumw\": sumw,\r\n",
    "            \"mass\": mass_hist\r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the data source that we will be running against."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def make_ds(name: str, query: ObjectStream):\r\n",
    "    '''Create a ServiceX Datasource for a particular ATLAS Open data file\r\n",
    "    '''\r\n",
    "    datasets = [ServiceXDataset(files[name]['files'], backend_name='dev_uproot')]\r\n",
    "    return DataSource(query=query, metadata={'dataset': name}, datasets=datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And run!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "analysis = ATLAS_Higgs_4L()\r\n",
    "# TODO: It would be good if datatype was determined automagically (there is enough info)\r\n",
    "executor = DaskExecutor('localhost:8786')\r\n",
    "datasource = make_ds('ggH125_ZZ4lep', leptons)\r\n",
    "\r\n",
    "async def run_updates_stream(accumulator_stream):\r\n",
    "  global first\r\n",
    "\r\n",
    "  count = 0\r\n",
    "  async for coffea_info in accumulator_stream:\r\n",
    "    count += 1\r\n",
    "    print(count, coffea_info)\r\n",
    "  return coffea_info\r\n",
    "\r\n",
    "# Why do I need run_updates_stream, why not just await on execute (which fails with async gen can't).\r\n",
    "# Perhaps something from aiostream can help here?\r\n",
    "result = await run_updates_stream(executor.execute(analysis, datasource))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7685dd7caee4a93a5cf4a271511a3ce"
      },
      "text/plain": [
       "[root://eospublic.ce...:   0%|          | 0/9000000000.0 [00:00]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Tried sending message after closing.  Status: closed\nMessage: {'op': 'update-graph-hlg', 'hlg': {'layers': [{'__module__': 'dask.highlevelgraph', '__name__': 'MaterializedLayer', 'state': {'dsk': {'run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e': {'function': b'\\x80\\x04\\x95?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\"coffea.processor.servicex.executor\\x94\\x8c\\x14run_coffea_processor\\x94\\x93\\x94.', 'args': b'\\x80\\x04]\\x94.', 'kwargs': b'\\x80\\x04\\x95\\xb0\\x06\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\nevents_url\\x94X\\xcf\\x01\\x00\\x00http://localhost:9000/fbb77aed-153c-4cc4-a749-3a82ed00e017/root%3A%3A%3Aeospublic.cern.ch%3A%3Aeos%3Aopendata%3Aatlas%3AOutreachDatasets%3A2020-01-22%3A4lep%3AMC%3Amc_345060.ggH125_ZZ4lep.4lep.root.parquet?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=miniouser%2F20210930%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210930T130216Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=c6a3151f39d095e21ba01d15f3ea36cd4f6a7b2755e8c89393ddb6a0867ecd64\\x94\\x8c\\ttree_name\\x94N\\x8c\\tdata_type\\x94\\x8c\\x07parquet\\x94\\x8c\\tmeta_data\\x94}\\x94\\x8c\\x07dataset\\x94\\x8ct[root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root]\\x94s\\x8c\\x04proc\\x94\\x8c\\x17cloudpickle.cloudpickle\\x94\\x8c\\r_builtin_type\\x94\\x93\\x94\\x8c\\nLambdaType\\x94\\x85\\x94R\\x94(h\\r\\x8c\\x08CodeType\\x94\\x85\\x94R\\x94(K\\x01K\\x00K\\x00K\\nK\\x0bKCC\\xc2d\\x01d\\x00l\\x00}\\x01d\\x01d\\x02l\\x01m\\x02}\\x02\\x01\\x00|\\x02t\\x03\\x83\\x01}\\x03t\\x04\\xa0\\x05d\\x03t\\x04\\xa0\\x06d\\x04d\\x05\\xa1\\x02t\\x04\\xa0\\x07d\\x06d\\x07d\\x08d\\x08d\\t\\xa1\\x05\\xa1\\x03}\\x04|\\x00j\\x08d\\x04\\x19\\x00}\\x05|\\x00j\\t}\\x06|\\x01\\xa0\\n|\\x06\\xa1\\x01d\\nk\\x02}\\x07|\\x06d\\x00d\\x00\\x85\\x02d\\x01f\\x02\\x19\\x00|\\x06d\\x00d\\x00\\x85\\x02d\\x0bf\\x02\\x19\\x00\\x17\\x00}\\x08|\\x06j\\x0b}\\t|\\x03|\\x05\\x05\\x00\\x19\\x00t\\x0c|\\x00\\x83\\x017\\x00\\x03\\x00<\\x00t\\rt\\x0c|\\x00\\x83\\x01\\x83\\x01\\x01\\x00|\\x04j\\x0e|\\x05|\\x01\\xa0\\x0f|\\t\\xa1\\x01d\\x0c\\x8d\\x02\\x01\\x00|\\x03|\\x04d\\r\\x9c\\x02S\\x00\\x94(NK\\x00\\x8c\\x0bdefaultdict\\x94\\x85\\x94\\x8c\\x06Events\\x94h\\x08\\x8c\\x07Dataset\\x94\\x8c\\x04mass\\x94\\x8c\\x0e$Z_{ee}$ [GeV]\\x94K<KxK\\x04K\\x01h\\x08h\\x19\\x86\\x94\\x8c\\x04sumw\\x94h\\x19\\x86\\x94t\\x94(\\x8c\\x07awkward\\x94\\x8c\\x0bcollections\\x94h\\x15\\x8c\\x05float\\x94\\x8c\\x04hist\\x94\\x8c\\x04Hist\\x94\\x8c\\x03Cat\\x94\\x8c\\x03Bin\\x94\\x8c\\x08metadata\\x94\\x8c\\x03lep\\x94\\x8c\\x03num\\x94h\\x19\\x8c\\x03len\\x94\\x8c\\x05print\\x94\\x8c\\x04fill\\x94\\x8c\\x07flatten\\x94t\\x94(\\x8c\\x06events\\x94\\x8c\\x02ak\\x94h\\x15h\\x1c\\x8c\\tmass_hist\\x94h\\x08\\x8c\\x07leptons\\x94\\x8c\\x03cut\\x94\\x8c\\x08dilepton\\x94\\x8c\\x07mass_4l\\x94t\\x94\\x8c@C:\\\\Users\\\\gordo\\\\AppData\\\\Local\\\\Temp/ipykernel_454932/1137201104.py\\x94\\x8c\\x07process\\x94K\\x02C,\\x00\\x02\\x08\\x01\\x0c\\x02\\x08\\x01\\x04\\x01\\x02\\x01\\n\\x01\\x10\\xfd\\x04\\x06\\n\\x01\\x06\\x03\\x0e\\x05 \\x01\\x06\\x03\\x14\\x01\\x0c\\x01\\x04\\x01\\x02\\x01\\x08\\xfe\\x06\\x06\\x02\\x01\\x02\\xfe\\x94))t\\x94R\\x94}\\x94(\\x8c\\x0b__package__\\x94N\\x8c\\x08__name__\\x94\\x8c\\x08__main__\\x94uNNNt\\x94R\\x94\\x8c\\x1ccloudpickle.cloudpickle_fast\\x94\\x8c\\x12_function_setstate\\x94\\x93\\x94h@}\\x94}\\x94(h=h7\\x8c\\x0c__qualname__\\x94\\x8c\\x16ATLAS_Higgs_4L.process\\x94\\x8c\\x0f__annotations__\\x94}\\x94\\x8c\\x0e__kwdefaults__\\x94N\\x8c\\x0c__defaults__\\x94N\\x8c\\n__module__\\x94h>\\x8c\\x07__doc__\\x94N\\x8c\\x0b__closure__\\x94N\\x8c\\x17_cloudpickle_submodules\\x94]\\x94\\x8c\\x0b__globals__\\x94}\\x94h\"h\\x0b\\x8c\\tsubimport\\x94\\x93\\x94\\x8c\\x0bcoffea.hist\\x94\\x85\\x94R\\x94su\\x86\\x94\\x86R0u.'}}, 'dependencies': {'run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e': set()}}, 'annotations': {}}]}, 'keys': ['run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e'], 'priority': {'run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e': 0}, 'submitting_task': None, 'fifo_timeout': '100 ms', 'actors': False, 'code': '    def run_async_analysis(\\n        self,\\n        file_url: str,\\n        tree_name: Optional[str],\\n        data_type: str,\\n        meta_data: Dict[str, str],\\n        process_func: Callable,\\n    ):\\n        \"\"\"Create a dask future for a dask task to run the analysis.\"\"\"\\n        data_result = self.dask.submit(\\n            run_coffea_processor,\\n            events_url=file_url,\\n            tree_name=tree_name,\\n            data_type=data_type,\\n            meta_data=meta_data,\\n            proc=process_func,\\n        )\\n\\n        return data_result\\n'}",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_454932/1846572108.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Why do I need run_updates_stream, why not just await on execute (which fails with async gen can't).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Perhaps something from aiostream can help here?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mrun_updates_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_454932/1846572108.py\u001b[0m in \u001b[0;36mrun_updates_stream\u001b[1;34m(accumulator_stream)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoffea_info\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maccumulator_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoffea_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gordo\\code\\iris-hep\\coffea\\coffea\\processor\\servicex\\executor.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, analysis, datasource, title)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# There is an accumulate pattern in the aiostream lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfinished_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gordo\\code\\iris-hep\\coffea\\coffea\\processor\\accumulator.py\u001b[0m in \u001b[0;36masync_accumulate\u001b[1;34m(result_stream)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;32masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miadd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\opendata-higgs-discovery\\.venv\\lib\\site-packages\\aiostream\\stream\\advanced.py\u001b[0m in \u001b[0;36mbase_combine\u001b[1;34m(source, switch, ordered, task_limit)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# Get result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# End of stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\opendata-higgs-discovery\\.venv\\lib\\site-packages\\aiostream\\stream\\combine.py\u001b[0m in \u001b[0;36msmap\u001b[1;34m(source, func, *more_sources)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmore_sources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mstreamcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmore_sources\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gordo\\code\\iris-hep\\coffea\\coffea\\processor\\servicex\\executor.py\u001b[0m in \u001b[0;36mlaunch_analysis_tasks_from_stream\u001b[1;34m(self, result_file_stream, meta_data, process_func)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;31m# Invoke the implementation's task launcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             data_result = self.run_async_analysis(\n\u001b[0m\u001b[0;32m    115\u001b[0m                 \u001b[0mfile_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mtree_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gordo\\code\\iris-hep\\coffea\\coffea\\processor\\servicex\\dask_executor.py\u001b[0m in \u001b[0;36mrun_async_analysis\u001b[1;34m(self, file_url, tree_name, data_type, meta_data, process_func)\u001b[0m\n\u001b[0;32m     61\u001b[0m     ):\n\u001b[0;32m     62\u001b[0m         \u001b[1;34m\"\"\"Create a dask future for a dask task to run the analysis.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         data_result = self.dask.submit(\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mrun_coffea_processor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mevents_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\opendata-higgs-discovery\\.venv\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, func, key, workers, resources, retries, priority, fifo_timeout, allow_other_workers, actor, actors, pure, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m             \u001b[0mdsk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mskey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1555\u001b[1;33m         futures = self._graph_to_futures(\n\u001b[0m\u001b[0;32m   1556\u001b[0m             \u001b[0mdsk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mskey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\opendata-higgs-discovery\\.venv\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36m_graph_to_futures\u001b[1;34m(self, dsk, keys, workers, allow_other_workers, priority, user_priority, resources, retries, fifo_timeout, actors)\u001b[0m\n\u001b[0;32m   2579\u001b[0m             \u001b[0mfutures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFuture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeyset\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2581\u001b[1;33m             self._send_to_scheduler(\n\u001b[0m\u001b[0;32m   2582\u001b[0m                 {\n\u001b[0;32m   2583\u001b[0m                     \u001b[1;34m\"op\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"update-graph-hlg\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gordo\\Code\\iris-hep\\opendata-higgs-discovery\\.venv\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36m_send_to_scheduler\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_to_scheduler_safe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             raise Exception(\n\u001b[0m\u001b[0;32m    952\u001b[0m                 \u001b[1;34m\"Tried sending message after closing.  Status: %s\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[1;34m\"Message: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Tried sending message after closing.  Status: closed\nMessage: {'op': 'update-graph-hlg', 'hlg': {'layers': [{'__module__': 'dask.highlevelgraph', '__name__': 'MaterializedLayer', 'state': {'dsk': {'run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e': {'function': b'\\x80\\x04\\x95?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\"coffea.processor.servicex.executor\\x94\\x8c\\x14run_coffea_processor\\x94\\x93\\x94.', 'args': b'\\x80\\x04]\\x94.', 'kwargs': b'\\x80\\x04\\x95\\xb0\\x06\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\nevents_url\\x94X\\xcf\\x01\\x00\\x00http://localhost:9000/fbb77aed-153c-4cc4-a749-3a82ed00e017/root%3A%3A%3Aeospublic.cern.ch%3A%3Aeos%3Aopendata%3Aatlas%3AOutreachDatasets%3A2020-01-22%3A4lep%3AMC%3Amc_345060.ggH125_ZZ4lep.4lep.root.parquet?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=miniouser%2F20210930%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210930T130216Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=c6a3151f39d095e21ba01d15f3ea36cd4f6a7b2755e8c89393ddb6a0867ecd64\\x94\\x8c\\ttree_name\\x94N\\x8c\\tdata_type\\x94\\x8c\\x07parquet\\x94\\x8c\\tmeta_data\\x94}\\x94\\x8c\\x07dataset\\x94\\x8ct[root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root]\\x94s\\x8c\\x04proc\\x94\\x8c\\x17cloudpickle.cloudpickle\\x94\\x8c\\r_builtin_type\\x94\\x93\\x94\\x8c\\nLambdaType\\x94\\x85\\x94R\\x94(h\\r\\x8c\\x08CodeType\\x94\\x85\\x94R\\x94(K\\x01K\\x00K\\x00K\\nK\\x0bKCC\\xc2d\\x01d\\x00l\\x00}\\x01d\\x01d\\x02l\\x01m\\x02}\\x02\\x01\\x00|\\x02t\\x03\\x83\\x01}\\x03t\\x04\\xa0\\x05d\\x03t\\x04\\xa0\\x06d\\x04d\\x05\\xa1\\x02t\\x04\\xa0\\x07d\\x06d\\x07d\\x08d\\x08d\\t\\xa1\\x05\\xa1\\x03}\\x04|\\x00j\\x08d\\x04\\x19\\x00}\\x05|\\x00j\\t}\\x06|\\x01\\xa0\\n|\\x06\\xa1\\x01d\\nk\\x02}\\x07|\\x06d\\x00d\\x00\\x85\\x02d\\x01f\\x02\\x19\\x00|\\x06d\\x00d\\x00\\x85\\x02d\\x0bf\\x02\\x19\\x00\\x17\\x00}\\x08|\\x06j\\x0b}\\t|\\x03|\\x05\\x05\\x00\\x19\\x00t\\x0c|\\x00\\x83\\x017\\x00\\x03\\x00<\\x00t\\rt\\x0c|\\x00\\x83\\x01\\x83\\x01\\x01\\x00|\\x04j\\x0e|\\x05|\\x01\\xa0\\x0f|\\t\\xa1\\x01d\\x0c\\x8d\\x02\\x01\\x00|\\x03|\\x04d\\r\\x9c\\x02S\\x00\\x94(NK\\x00\\x8c\\x0bdefaultdict\\x94\\x85\\x94\\x8c\\x06Events\\x94h\\x08\\x8c\\x07Dataset\\x94\\x8c\\x04mass\\x94\\x8c\\x0e$Z_{ee}$ [GeV]\\x94K<KxK\\x04K\\x01h\\x08h\\x19\\x86\\x94\\x8c\\x04sumw\\x94h\\x19\\x86\\x94t\\x94(\\x8c\\x07awkward\\x94\\x8c\\x0bcollections\\x94h\\x15\\x8c\\x05float\\x94\\x8c\\x04hist\\x94\\x8c\\x04Hist\\x94\\x8c\\x03Cat\\x94\\x8c\\x03Bin\\x94\\x8c\\x08metadata\\x94\\x8c\\x03lep\\x94\\x8c\\x03num\\x94h\\x19\\x8c\\x03len\\x94\\x8c\\x05print\\x94\\x8c\\x04fill\\x94\\x8c\\x07flatten\\x94t\\x94(\\x8c\\x06events\\x94\\x8c\\x02ak\\x94h\\x15h\\x1c\\x8c\\tmass_hist\\x94h\\x08\\x8c\\x07leptons\\x94\\x8c\\x03cut\\x94\\x8c\\x08dilepton\\x94\\x8c\\x07mass_4l\\x94t\\x94\\x8c@C:\\\\Users\\\\gordo\\\\AppData\\\\Local\\\\Temp/ipykernel_454932/1137201104.py\\x94\\x8c\\x07process\\x94K\\x02C,\\x00\\x02\\x08\\x01\\x0c\\x02\\x08\\x01\\x04\\x01\\x02\\x01\\n\\x01\\x10\\xfd\\x04\\x06\\n\\x01\\x06\\x03\\x0e\\x05 \\x01\\x06\\x03\\x14\\x01\\x0c\\x01\\x04\\x01\\x02\\x01\\x08\\xfe\\x06\\x06\\x02\\x01\\x02\\xfe\\x94))t\\x94R\\x94}\\x94(\\x8c\\x0b__package__\\x94N\\x8c\\x08__name__\\x94\\x8c\\x08__main__\\x94uNNNt\\x94R\\x94\\x8c\\x1ccloudpickle.cloudpickle_fast\\x94\\x8c\\x12_function_setstate\\x94\\x93\\x94h@}\\x94}\\x94(h=h7\\x8c\\x0c__qualname__\\x94\\x8c\\x16ATLAS_Higgs_4L.process\\x94\\x8c\\x0f__annotations__\\x94}\\x94\\x8c\\x0e__kwdefaults__\\x94N\\x8c\\x0c__defaults__\\x94N\\x8c\\n__module__\\x94h>\\x8c\\x07__doc__\\x94N\\x8c\\x0b__closure__\\x94N\\x8c\\x17_cloudpickle_submodules\\x94]\\x94\\x8c\\x0b__globals__\\x94}\\x94h\"h\\x0b\\x8c\\tsubimport\\x94\\x93\\x94\\x8c\\x0bcoffea.hist\\x94\\x85\\x94R\\x94su\\x86\\x94\\x86R0u.'}}, 'dependencies': {'run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e': set()}}, 'annotations': {}}]}, 'keys': ['run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e'], 'priority': {'run_coffea_processor-5027d76ce086b8afd788b5bb47625a7e': 0}, 'submitting_task': None, 'fifo_timeout': '100 ms', 'actors': False, 'code': '    def run_async_analysis(\\n        self,\\n        file_url: str,\\n        tree_name: Optional[str],\\n        data_type: str,\\n        meta_data: Dict[str, str],\\n        process_func: Callable,\\n    ):\\n        \"\"\"Create a dask future for a dask task to run the analysis.\"\"\"\\n        data_result = self.dask.submit(\\n            run_coffea_processor,\\n            events_url=file_url,\\n            tree_name=tree_name,\\n            data_type=data_type,\\n            meta_data=meta_data,\\n            proc=process_func,\\n        )\\n\\n        return data_result\\n'}"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hist.plot1d(result['mass'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}