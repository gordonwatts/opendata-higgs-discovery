{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Lets load our environment first"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from func_adl_servicex import ServiceXSourceUpROOT, ServiceXSourceCMSRun1AOD\n",
    "from hist import Hist\n",
    "import awkward as ak"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flat ROOT Files\n",
    "\n",
    "ATLAS has distributed it's open data as flat ROOT files.\n",
    "\n",
    "* On CERNOpenData they are a single zip file\n",
    "* But they have been distributed as files available via EOS from CERN Open Data's EOS instance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ggH125_ZZ4lep = 'root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root'\n",
    "# ggH125_ZZ4lep = 'https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root'\n",
    "ggH125_ZZ4lep_source = ServiceXSourceUpROOT([ggH125_ZZ4lep], 'mini','uproot-af')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-2-b74bcdddd3ba>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-b74bcdddd3ba>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ggH125_ZZ4lep = 'root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/MC/mc_345060.ggH125_ZZ4lep.4lep.root''\u001b[0m\n\u001b[1;37m                                                                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We use the `root://` address instead of `http://` due to efficiency and caching.\n",
    "* `mini` is the tree name in the file\n",
    "* last parameter basically describes the type of file - this is a flat root file that can be opened by the `uproot` python package.\n",
    "\n",
    "Now that we have a reference to the datasource, lets pick out a single column and bring its contents back to our local instance:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from servicex import ignore_cache\n",
    "\n",
    "with ignore_cache():\n",
    "    r = (ggH125_ZZ4lep_source\n",
    "         .Select(lambda e: {'lep_pt': e['lep_pt']})\n",
    "         .AsAwkwardArray()\n",
    "         .value()\n",
    "        )\n",
    "r"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a standard awkward array - and it's shape is simply a series of lepton $p_T$'s for each of the 164716 events:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r.type"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot this. We'll use the `Hist` library, which is a nice wrapper around the `boost_histogram` library:\n",
    "* Note we have a single axis, which is the muon's $p_T$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h = (Hist.new\n",
    "     .Reg(50, 0, 200, name='mu_pt', label='Muon $p_T$ [GeV]')\n",
    "     .Int64()\n",
    "     )\n",
    "h.fill(ak.flatten(r['lep_pt'])/1000.0)\n",
    "_ = h.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets get back a $p_T$ and an $\\eta$ for the leptons now. This requires two things coming back:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r = (ggH125_ZZ4lep_source\n",
    "     .Select(lambda e: {'lep_pt': e['lep_pt'], 'lep_eta': e['lep_eta']})\n",
    "     .AsAwkwardArray()\n",
    "     .value()\n",
    "    )\n",
    "r.type"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that it is columnar data:\n",
    "\n",
    "* Each event contains two arrays\n",
    "* The arrays are lepton pt and eta - not a tuple of lepton ($p_T$, $\\eta$).\n",
    "\n",
    "What if we want to cut on the eta? How do we relate these two columns? We use the zip function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r_cut = (ggH125_ZZ4lep_source\n",
    "         .Select(lambda e: Zip({'pt': e['lep_pt'], 'eta': e['lep_eta']}))\n",
    "         .Select(lambda leps: leps.Where(lambda l: abs(l.eta) < 1.0))\n",
    "         .AsAwkwardArray()\n",
    "         .value()\n",
    "        )\n",
    "r.type"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interesting - same number of events - but we cut? Lets look at a histogram:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h = (Hist.new\n",
    "     .Reg(50, 0, 200, name='mu_pt', label='Muon Track $p_T$ [GeV]')\n",
    "     .StrCat([], name='cut', label='Cut Type', growth=True)\n",
    "     .Int64()\n",
    "     )\n",
    "h.fill(mu_pt=ak.flatten(r['lep_pt'])/1000.0, cut='All')\n",
    "h.fill(mu_pt=ak.flatten(r_cut['pt'])/1000.0, cut='eta')\n",
    "artists = h.plot()\n",
    "\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h = (Hist.new\n",
    "     .Reg(8, 0, 8, name='muon_count', label='Number of Muons')\n",
    "     .StrCat([], name='cut', label='Cut Type', growth=True)\n",
    "     .Int64()\n",
    "     )\n",
    "h.fill(muon_count=ak.count(r['lep_pt'], axis=-1), cut='All')\n",
    "h.fill(muon_count=ak.count(r_cut['pt'], axis=-1), cut='eta')\n",
    "artists = h.plot()\n",
    "\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CMS Run 1 AOD Files\n",
    "\n",
    "Differences between FLAT ROOT files and CMS RUN 1 AOD Files:\n",
    "\n",
    "* You must use an (old) version of CMSSW, a big software framework, to read these files!!\n",
    "* Data in these files is stored event or row-wise: electrons, and then the $p_T$ and $\\eta$ of each electron.\n",
    "* Some datasets are ~7 TB!! It takes about 30 minutes to run on those when things are working well: we will be using a data from an earlier run that has automatically been locally cached.\n",
    "\n",
    "Start with a 60GB SM [Higgs dataset](http://opendata.cern.ch/record/1507) ($H \\rightarrow ZZ \\rightarrow \\ell \\ell \\ell \\ell$). In CERN OpenData's catalog, this is record 1507 (pulled straight from the URL: http://opendata.cern.ch/record/1507)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We cut the number of leptons per event\n",
    "* We now have some events with zero leptons - but those events were still returned\n",
    "* We could remove them by doing a cut on the number of leptons..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hzz_4l_source = ServiceXSourceCMSRun1AOD('cernopendata://1507', 'cms_run1_aod')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r = (hzz_4l_source \\\n",
    "     .Select(lambda e: e.TrackMuons(\"globalMuons\"))\n",
    "     .Select(lambda muons: muons.Select(lambda m: m.pt()))\n",
    "     .AsAwkwardArray(['mu_pt']) \\\n",
    "     .value()\n",
    "    )\n",
    "r.type"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r_cut = (hzz_4l_source \\\n",
    "         .Select(lambda e: e.TrackMuons(\"globalMuons\"))\n",
    "         .Select(lambda muons: muons.Where(lambda m: abs(m.eta()) < 1.0))\n",
    "         .Select(lambda muons: muons.Select(lambda m: m.pt()))\n",
    "         .AsAwkwardArray(['mu_pt']) \\\n",
    "         .value()\n",
    "    )\n",
    "r.type"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h = (Hist.new\n",
    "     .Reg(8, 0, 8, name='muon_count', label='Number of Muons')\n",
    "     .StrCat([], name='cut', label='Cut Type', growth=True)\n",
    "     .Int64()\n",
    "     )\n",
    "h.fill(muon_count=ak.count(r['mu_pt'], axis=-1), cut='All')\n",
    "h.fill(muon_count=ak.count(r_cut['mu_pt'], axis=-1), cut='eta')\n",
    "artists = h.plot()\n",
    "\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, a very similar behavior here!\n",
    "\n",
    "* Note that CMS muons are **all** muons, so a lot more quality cuts must be done to compare them with ATLAS's muons.\n",
    "* ATLAS's AOD files were skimmed down to make those datasets for the educational purposes: so a lot of the skimming is done early for those files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Coffea\n",
    "\n",
    "`ServiceX`:\n",
    "\n",
    "* Gets columnar data from any format that can a translator has been written for (ATLAS Run 2 xAOD, CMS Run 1 AOD, uproot-able ROOT files, soon some dark matter experiments, etc.)\n",
    "* Slims, skims, generates calculated quantities\n",
    "\n",
    "Think _ntuplizer_.\n",
    "\n",
    "`coffea`:\n",
    "\n",
    "* Used `awkward` and friends to perform the final analysis\n",
    "* Plotting\n",
    "* Distributed running\n",
    "* Good at running on a large number of datasets at once\n",
    "\n",
    "`coffea` is arranged around processors that do the physics. Each processor runs once per file, and results are combined for a dataset.\n",
    "\n",
    "First define a dummy (representative) dataset and apply the operations on it we are interested in:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = ServiceXSourceUpROOT('cernopendata://dummy',  \"mimi\", 'uproot')\n",
    "ds.return_qastle = True  # Magic\n",
    "\n",
    "selection_atlas = (ds\n",
    "                     .Select(lambda e: Zip({'lep_pt': e['lep_pt'], 'lep_eta': e['lep_eta']}))\n",
    "                     .Select(lambda leps: leps.Where(lambda l: abs(l.lep_eta) < 1.0))\n",
    "                     .Select(lambda leps: {'lep_pt': leps.lep_pt, 'lep_eta': leps.lep_eta})\n",
    "                     .AsParquetFiles('junk.parquet')\n",
    "                  )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note:\n",
    "\n",
    "* no call to `value`: We do not want to try to render this bogus expression.\n",
    "* We want to return parquet files - this is what `coffea` will be eating and sending to the processors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from coffea.processor.servicex import DataSource, Analysis\n",
    "from coffea.processor.servicex import LocalExecutor\n",
    "from servicex import ServiceXDataset\n",
    "from coffea import processor\n",
    "\n",
    "class atlas_demo_processor(Analysis):\n",
    "    @staticmethod\n",
    "    def process(events):\n",
    "        import awkward as ak\n",
    "        from collections import defaultdict\n",
    "\n",
    "        sumw = defaultdict(float)\n",
    "        h = (Hist.new\n",
    "             .Reg(50, 0, 200, name='lep_pt', label='Lepton $p_T$ [GeV]')\n",
    "             .StrCat([], name='dataset', label='Dataset', growth=True)\n",
    "             .Int64()\n",
    "             )\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "        leptons = events.lep\n",
    "\n",
    "        h.fill(\n",
    "            dataset=dataset,\n",
    "            lep_pt = ak.flatten(leptons.pt/1000.0)\n",
    "        )\n",
    "                \n",
    "        return {\n",
    "            \"sumw\": sumw,\n",
    "            \"pt\": h\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we create a real dataset and execute it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datasets = [ServiceXDataset([ggH125_ZZ4lep], backend_name='uproot',\n",
    "                           image='sslhep/servicex_func_adl_uproot_transformer:pr_fix_awk_bug')]\n",
    "c_datasets = DataSource(query=selection_atlas, metadata={'dataset': 'ggH125_ZZ4lep'}, datasets=datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code is boiler plate:\n",
    "\n",
    "* Declare a dataset and a transformer image (were accidentally running an old version of awkward)\n",
    "* Create a datasource\n",
    "* Note the metadata - a useful way to pass information into your processor on a per-dataset basis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "analysis = atlas_demo_processor()\n",
    "executor = LocalExecutor()#datatype='parquet')\n",
    "\n",
    "async def run_updates_stream(accumulator_stream):\n",
    "  count = 0\n",
    "  async for coffea_info in accumulator_stream:\n",
    "    count += 1\n",
    "  return coffea_info\n",
    "\n",
    "result = await run_updates_stream(executor.execute(analysis, c_datasets))\n",
    "print(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "artists = result['pt'].plot()\n",
    "\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "f2575392019334285e0602a4035eec46b9260ee4c95297ea34ade6e3c8b8fcaf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}