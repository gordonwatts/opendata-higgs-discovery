{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ATLAS $H \\rightarrow ZZ \\rightarrow \\ell \\ell \\ell \\ell$ Public Outreach Example\n",
    "\n",
    "ATLAS has released its higgs discovery dataset as public data:\n",
    "\n",
    "* Use `ServiceX` to stream the 4 lepton data\n",
    "* Use `coffea` and `awkward` to produce the final $m_{4\\ell}$ plots.\n",
    "* This is **only** aboput 0.5 GB worth of data!\n",
    "\n",
    "Outline\n",
    "\n",
    "1. Use `ServiceX` for general quality and object selection\n",
    "1. Use `coffea` and `awkward` to do multi-object event wide selection and plots\n",
    "1. Produce the plot for running on a single MC file\n",
    "1. Run on all the MC and Data files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "from servicex.servicex import ServiceXDataset\n",
    "from coffea.processor.servicex import Analysis, DataSource, LocalExecutor\n",
    "from func_adl import ObjectStream\n",
    "from hist import Hist\n",
    "\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selecting events and clean objects\n",
    "\n",
    "The ATLAS analysis ...\n",
    "\n",
    "First we create the representative data source, and apply the initial trigger requirement:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ds = ServiceXSourceUpROOT('cernopendata://dummy',  \"mimi\", 'uproot')\n",
    "ds.return_qastle = True  # Magic\n",
    "\n",
    "good_events = (ds\n",
    "               .Where(lambda e: e.trigE or e.trigM)\n",
    "              )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, basic lepton selection:\n",
    "\n",
    "* Turn the columnar representation into object so we can make cuts\n",
    "* Apply the common base cuts for electrons and muons\n",
    "* We also need the event weights, which are baked into the ntuple"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "all_leptons = (good_events\n",
    "                .Select(lambda e: (\n",
    "                    Zip({\n",
    "                        'lep_pt': e.lep_pt,\n",
    "                        'lep_eta': e.lep_eta,\n",
    "                        'lep_phi': e.lep_phi,\n",
    "                        'lep_energy': e.lep_E,\n",
    "                        'lep_charge': e.lep_charge,\n",
    "                        'lep_ptcone30': e.lep_ptcone30,\n",
    "                        'lep_etcone20': e.lep_etcone20,\n",
    "                        'lep_typeid': e.lep_type,\n",
    "                        'lep_trackd0pvunbiased': e.lep_trackd0pvunbiased,\n",
    "                        'lep_tracksigd0pvunbiased': e.lep_tracksigd0pvunbiased,\n",
    "                        'lep_z0': e.lep_z0,\n",
    "                    }),\n",
    "                    e.mcWeight,\n",
    "                    e.scaleFactor_ELE*e.scaleFactor_MUON*e.scaleFactor_LepTRIGGER*e.scaleFactor_PILEUP,\n",
    "                ))\n",
    "               )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "good_leptons = (all_leptons\n",
    "                .Select(lambda e: {\n",
    "                    'ele': e[0]\n",
    "                           .Where(lambda lep: lep.lep_typeid == 11)\n",
    "                           .Where(lambda lep: (lep.lep_pt > 7000)\n",
    "                                              and (abs(lep.lep_eta) < 2.47)\n",
    "                                              and (lep.lep_etcone20/lep.lep_pt < 0.3)\n",
    "                                              and (lep.lep_ptcone30/lep.lep_pt < 0.3)\n",
    "                                              and (abs(lep.lep_trackd0pvunbiased) / lep.lep_tracksigd0pvunbiased < 5)\n",
    "                                 ),\n",
    "                    'mu': e[0]\n",
    "                           .Where(lambda lep: lep.lep_typeid == 13)\n",
    "                           .Where(lambda lep: (lep.lep_pt > 5000)\n",
    "                                              and (abs(lep.lep_eta) < 2.5)\n",
    "                                              and (lep.lep_etcone20/lep.lep_pt < 0.3)\n",
    "                                              and (lep.lep_ptcone30/lep.lep_pt < 0.3)\n",
    "                                              and (abs(lep.lep_trackd0pvunbiased) / lep.lep_tracksigd0pvunbiased < 3)\n",
    "                                 ),\n",
    "                    'mcWeight': e[1],\n",
    "                    'scaleFactor': e[2],\n",
    "                })\n",
    "               )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And finally, we have to turn this into a form that coffea can currently understand.\n",
    "\n",
    "* We only need to feed columns we will use downstream out of `ServiceX`, reducing the dataload."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "atlas_selection = (good_leptons\n",
    "                   .Select(lambda e: {\n",
    "                       'electrons_pt': e.ele.lep_pt,\n",
    "                       'electrons_eta': e.ele.lep_eta,\n",
    "                       'electrons_phi': e.ele.lep_phi,\n",
    "                       'electrons_energy': e.ele.lep_energy,\n",
    "                       'electrons_charge': e.ele.lep_charge,\n",
    "                       'electrons_z0': e.ele.lep_z0,\n",
    "                       'muons_pt': e.mu.lep_pt,\n",
    "                       'muons_eta': e.mu.lep_eta,\n",
    "                       'muons_phi': e.mu.lep_phi,\n",
    "                       'muons_energy': e.mu.lep_energy,\n",
    "                       'muons_charge': e.mu.lep_charge,\n",
    "                       'muons_z0': e.mu.lep_z0,\n",
    "                       'mcWeight': e.mcWeight,\n",
    "                       'scaleFactor': e.scaleFactor,\n",
    "                   })\n",
    "                   .AsParquetFiles('junk.parquet')\n",
    "                  )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performing the analysis\n",
    "\n",
    "The data from `ServiceX` is now analyzed by `awkward` and `coffea`.\n",
    "\n",
    "* `coffea` automatically builds _event_ layout for electorns and muons, queuing off the prefix in the names from `ServiceX`\n",
    "* `coffea` recognizes that $p_T$, $\\eta$, $\\phi$, and $E$ are availible, and builds a 4-vector that has a `theta` property.\n",
    "* `awkward` properly translates the `np.sin` to work on an `awkward` array."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ATLAS_Higgs_4L(Analysis):\n",
    "    '''Run the 4 Lepton analysis on ATLAS educational ntuples\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def process(events):\n",
    "        from collections import defaultdict\n",
    "        import numpy as np\n",
    "\n",
    "        import awkward as ak\n",
    "\n",
    "        sumw = defaultdict(float)\n",
    "        mass_hist = (Hist.new\n",
    "                     .Reg(60, 60, 180, name='mass', label='$m_{4\\ell}$ [GeV]')\n",
    "                     .StrCat([], name='dataset', label='Cut Type', growth=True)\n",
    "                     .StrCat([], name='channel', label='Channel', growth=True)\n",
    "                     .Int64()\n",
    "                    )\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "        electrons = events.electrons\n",
    "        muons = events.muons\n",
    "        \n",
    "        weight =  ak.Array(np.ones(len(events.scaleFactor))) if events.metadata['is_data'] \\\n",
    "            else events.scaleFactor*events.mcWeight\n",
    "\n",
    "        # We didn't have the 4-vector in `ServiceX`, so we couldn't do the final good-object cut.\n",
    "        # Good electon selection\n",
    "        electrons_mask = (abs(electrons.z0*np.sin(electrons.theta)) < 0.5)\n",
    "        electrons_good = electrons[electrons_mask]\n",
    "\n",
    "        # Good muon selection\n",
    "        muons_mask = (abs(muons.z0*np.sin(muons.theta)) < 0.5)\n",
    "        muons_good = muons[muons_mask]\n",
    "\n",
    "        # Next are event level cuts\n",
    "        \n",
    "        # In order to cut in sorted lepton pt, we have to rebuild a lepton array here\n",
    "        leptons_good = ak.concatenate((electrons_good, muons_good), axis=1)\n",
    "        leptons_good_index = ak.argsort(leptons_good.pt, ascending=False)\n",
    "        leptons_good_sorted = leptons_good[leptons_good_index]\n",
    "\n",
    "        # Event level cuts now that we know the good leptons\n",
    "        # - We need to look at 4 good lepton events only\n",
    "        # - We need same flavor, so check for even numbers of each flavor\n",
    "        # - all charges must be balenced\n",
    "        event_mask = (\n",
    "            (ak.num(leptons_good_sorted) == 4)\n",
    "            & ((ak.num(electrons_good) == 0) | (ak.num(electrons_good) == 2) | (ak.num(electrons_good) == 4))\n",
    "            & ((ak.num(muons_good) == 0) | (ak.num(muons_good) == 2) | (ak.num(muons_good) == 4))\n",
    "            & (ak.sum(electrons_good.charge, axis=1) == 0)\n",
    "            & (ak.sum(muons_good.charge, axis=1) == 0)\n",
    "        )\n",
    "        \n",
    "        # Next, we need to cut on the pT for the leading, sub-leading, and sub-sub-leading lepton\n",
    "        leptons_good_preselection = leptons_good[event_mask]\n",
    "        event_good_lepton_mask = (\n",
    "            (leptons_good_preselection[:,0].pt > 25000.0)\n",
    "            & (leptons_good_preselection[:,1].pt > 15000.0)\n",
    "            & (leptons_good_preselection[:,2].pt > 10000.0)\n",
    "        )\n",
    "\n",
    "        # Now, we need to rebuild the good muon and electron lists with those selections\n",
    "        muons_analysis = muons_good[event_mask][event_good_lepton_mask]\n",
    "        electrons_analysis = electrons_good[event_mask][event_good_lepton_mask]\n",
    "\n",
    "        # Lets do eemumu events - as there are no permutations there.abs\n",
    "        # At this point if there are two muons, there must be two electrons\n",
    "        eemumu_mask = (ak.num(muons_analysis) == 2)\n",
    "        muon_eemumu = muons_analysis[eemumu_mask]\n",
    "        electrons_eemumu = electrons_analysis[eemumu_mask]\n",
    "        z1_eemumu = muon_eemumu[:,0] + muon_eemumu[:,1]\n",
    "        z2_eemumu = electrons_eemumu[:,0] + electrons_eemumu[:,1]\n",
    "        h_eemumu = z1_eemumu + z2_eemumu\n",
    "\n",
    "        sumw[dataset] += len(h_eemumu)\n",
    "        mass_hist.fill(\n",
    "            channel=r'$ee\\mu\\mu$',\n",
    "            mass=h_eemumu.mass/1000.0,\n",
    "            dataset=dataset,\n",
    "#             weight=weight[eemumu_mask]\n",
    "        )\n",
    "\n",
    "        # Next, eeee. For this we have to build permutations and select the best one\n",
    "        def four_leptons_one_flavor(same_flavor_leptons, event_weights, channel: str):\n",
    "            fl_positive = same_flavor_leptons[same_flavor_leptons.charge > 0]\n",
    "            fl_negative = same_flavor_leptons[same_flavor_leptons.charge < 0]\n",
    "            fl_pairs = ak.cartesian((fl_positive, fl_negative))\n",
    "            zs = fl_pairs[\"0\"] + fl_pairs[\"1\"]\n",
    "\n",
    "            delta = abs((91.18*1000.0) - zs.mass[:])\n",
    "            closest_masses = np.min(delta, axis=-1)\n",
    "            the_closest = (delta == closest_masses)\n",
    "            the_furthest = the_closest[:,::-1]\n",
    "\n",
    "            h_eeee = zs[the_closest] + zs[the_furthest]\n",
    "            sumw[dataset] += len(h_eeee)\n",
    "            mass_hist.fill(\n",
    "                channel=channel,\n",
    "                mass=ak.flatten(h_eeee.mass/1000.0),\n",
    "                dataset=dataset,\n",
    "#                 weight=event_weights,\n",
    "            )\n",
    "\n",
    "        four_leptons_one_flavor(electrons_analysis[(ak.num(electrons_analysis) == 4)],\n",
    "                                weight[(ak.num(electrons_analysis) == 4)],\n",
    "                                '$eeee$')\n",
    "        four_leptons_one_flavor(muons_analysis[(ak.num(muons_analysis) == 4)],\n",
    "                                weight[(ak.num(muons_analysis) == 4)],\n",
    "                                '$\\\\mu\\\\mu\\\\mu\\\\mu$')\n",
    "        \n",
    "        return {\n",
    "            \"sumw\": sumw,\n",
    "            \"mass\": mass_hist,\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run on a MC file\n",
    "\n",
    "Define a convience function to load MC files - there are a lot of them.\n",
    "\n",
    "* Note the `is_data` metadata - which was used above in an if statement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def make_ds(name: str, query: ObjectStream):\n",
    "    '''Create a ServiceX Datasource for a particular ATLAS Open data file\n",
    "    '''\n",
    "    from utils import files\n",
    "    is_data = name == 'data'\n",
    "    datasets = [ServiceXDataset(files[name]['files'], 'uproot', image='sslhep/servicex_func_adl_uproot_transformer:pr_fix_awk_bug')]\n",
    "    return DataSource(query=query, metadata={'dataset': name, 'is_data': is_data}, datasets=datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from utils import files\n",
    "all_datasets = list(files.keys())\n",
    "', '.join(all_datasets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'data, ggH125_ZZ4lep, ZH125_ZZ4lep, VBFH125_ZZ4lep, WH125_ZZ4lep, ZqqZll, WpqqWmlv, WplvWmqq, WlvZqq, llll, lllv, llvv, lvvv, Zee, Zmumu, Ztautau'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And an easy routine that will run a single file\n",
    "\n",
    "* Async so we can run multiple queires at once\n",
    "* Contains a lot of boiler plate\n",
    "* Runs on multiple datasets at once (which we will need)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "async def run_analysis(names: List[str]):\n",
    "    'Generate base plot for a multiple datafiles'\n",
    "\n",
    "    executor = LocalExecutor()\n",
    "    # executor = LocalExecutor(datatype='parquet')\n",
    "    datasources = [make_ds(ds_name, atlas_selection) for ds_name in names]\n",
    "\n",
    "    # Create the analysis and we can run from there.\n",
    "    analysis = ATLAS_Higgs_4L()\n",
    "\n",
    "    async def run_updates_stream(accumulator_stream, name):\n",
    "        '''Run to get the last item in the stream, with a useful error message'''\n",
    "        coffea_info = None\n",
    "        try:\n",
    "            async for coffea_info in accumulator_stream:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Failure while processing {name}') from e\n",
    "        return coffea_info\n",
    "\n",
    "    # Run on all items and wait till they are done!\n",
    "    all_plots = await asyncio.gather(*[run_updates_stream(executor.execute(analysis, source), source.metadata['dataset']) for source in datasources])\n",
    "    \n",
    "    all_plots_mass = [p['mass'] for p in all_plots]\n",
    "    mass = all_plots_mass[0]\n",
    "    for p in all_plots_mass[1:]:\n",
    "        mass += p\n",
    "\n",
    "    return mass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "mc_mass_plot = await run_analysis(['ggH125_ZZ4lep'])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Failure while processing ggH125_ZZ4lep",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mServiceXException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e2c9ff2a8de1>\u001b[0m in \u001b[0;36mrun_updates_stream\u001b[1;34m(accumulator_stream, name)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoffea_info\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maccumulator_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\coffea\\processor\\servicex\\executor.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, analysis, datasource, title)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfinished_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\coffea\\processor\\accumulator.py\u001b[0m in \u001b[0;36masync_accumulate\u001b[1;34m(result_stream)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\aiostream\\stream\\advanced.py\u001b[0m in \u001b[0;36mbase_combine\u001b[1;34m(source, switch, ordered, task_limit)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\aiostream\\stream\\combine.py\u001b[0m in \u001b[0;36msmap\u001b[1;34m(source, func, *more_sources)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32masync\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mstreamcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmore_sources\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\coffea\\processor\\servicex\\executor.py\u001b[0m in \u001b[0;36mlaunch_analysis_tasks_from_stream\u001b[1;34m(self, result_file_stream, meta_data, process_func)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mtree_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msx_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_file_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[0mfile_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msx_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\coffea\\processor\\servicex\\data_source.py\u001b[0m in \u001b[0;36mstream_result_files\u001b[1;34m(self, title)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"parquet\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data_parquet_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqastle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_as_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\servicex\\servicex.py\u001b[0m in \u001b[0;36mget_data_parquet_stream\u001b[1;34m(self, selection_query, title)\u001b[0m\n\u001b[0;32m    319\u001b[0m         '''\n\u001b[1;32m--> 320\u001b[1;33m         async for f_info in self._stream_local_files(selection_query, title,\n\u001b[0m\u001b[0;32m    321\u001b[0m                                                      'parquet'):  # type: ignore\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\servicex\\servicex.py\u001b[0m in \u001b[0;36m_stream_local_files\u001b[1;34m(self, selection_query, title, data_format)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m         \u001b[1;32masync\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mas_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mStreamInfoPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mawait\u001b[0m \u001b[0ma_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\servicex\\servicex.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[0mas_files\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             (f async for f in\n\u001b[0m\u001b[0;32m    596\u001b[0m              self._get_files(selection_query, data_format, notifier, title))  # type: ignore\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\servicex\\servicex.py\u001b[0m in \u001b[0;36m_get_files\u001b[1;34m(self, selection_query, data_type, notifier, title)\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;31m# Get a request id - which might be cached, but if not, submit it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             \u001b[0mrequest_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_request_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\servicex\\servicex.py\u001b[0m in \u001b[0;36m_get_request_id\u001b[1;34m(self, client, query)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrequest_id\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mrequest_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_servicex_adaptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'request_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\servicex\\servicex_adaptor.py\u001b[0m in \u001b[0;36msubmit_query\u001b[1;34m(self, client, json_query)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 raise ServiceXException('ServiceX rejected the transformation request: '\n\u001b[0m\u001b[0;32m     67\u001b[0m                                         f'({status}){t}')\n",
      "\u001b[1;31mServiceXException\u001b[0m: (ServiceXException(...), 'ServiceX rejected the transformation request: (400){\"message\": \"Failed to submit transform request: Failed to generate translation code: Unknown id: ResultParquet\"}\\n')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8492f7fb4301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmc_mass_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0mrun_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ggH125_ZZ4lep'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-e2c9ff2a8de1>\u001b[0m in \u001b[0;36mrun_analysis\u001b[1;34m(names)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Run on all items and wait till they are done!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mall_plots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrun_updates_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasources\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mall_plots_mass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mass'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_plots\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e2c9ff2a8de1>\u001b[0m in \u001b[0;36mrun_updates_stream\u001b[1;34m(accumulator_stream, name)\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Failure while processing {name}'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcoffea_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Failure while processing ggH125_ZZ4lep"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "artists = mc_mass_plot.project('mass','channel').plot(stack=True)\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mc_mass_plot.project('mass').plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running on all MC and Data Files\n",
    "\n",
    "Here we will repeat the above, but unleash it on all our datasets. These will all be put into a single histogram, with the `dataset` bin marking what sample they are from."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mass_plot = await run_analysis(all_datasets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot everything - not that this is interesting from a physics point of view, but it does make sure everything was added to the histogram in the end!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "artists = mass_plot.project('mass', 'dataset').plot(stack=True)\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, lets look at the components. We'd like signal, data, and the sum of everything else (MC prediction).\n",
    "\n",
    "Data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mass_plot[:, 'data', :].project('mass').plot();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Signal:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "signal_ds = list(d for d in all_datasets if 'H125' in d)\n",
    "\n",
    "artists = mass_plot[:,signal_ds,:].project('mass', 'dataset').plot(stack=True)\n",
    "\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sum of the backgrounds:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mc_ds = list(str(i) for i in (set(list(mass_plot.axes[1])) - set(signal_ds) - set(['data'])))\n",
    "mc_ds.sort()\n",
    "\n",
    "artists = mass_plot[:,mc_ds,:].project('mass', 'dataset').plot(stack=True)\n",
    "\n",
    "ax = artists[0].stairs.axes  # get the axis\n",
    "ax.legend(loc=\"best\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "f2575392019334285e0602a4035eec46b9260ee4c95297ea34ade6e3c8b8fcaf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}